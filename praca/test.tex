\documentclass[12pt,oneside,a4paper]{report}
\usepackage{mathtools}
\usepackage[HTML, hyperref, svgnames, table]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{polski}
\usepackage{multicol}
\usepackage{graphicx} 
\usepackage[margin=1in]{geometry}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{algpseudocode}
\usepackage[]{algorithm2e}
\usepackage{diagbox}
\usepackage{makecell}
\usepackage[justification=centering]{caption}
\usepackage[section]{placeins}

\begin{document}
\begin{titlepage}
   \begin{center}
        \vspace*{2cm}

        \Huge Politechnika Wrocławska

        \Huge Wydział Elektroniki
        ---------------------------------------------------------

   \end{center}
        \setlength{\columnsep}{-200pt}
        \begin{multicols}{2}
            \large KIERUNEK:


            \large SPECIALNOŚĆ: 


        \columnbreak
           Teleinformatyka

            Projektowanie Sieci Teleinformatycznych
        \end{multicols}

        \vspace{1cm}
   \begin{center}

      \huge \textbf{PRACA DYPLOMOWA}

   \huge \textbf{INŻYNIERKSA}


   \vspace{1.5cm}
   
         


            \large tu jest tytuł

            \vspace{0.5cm}

            \large tu tytuł w jezyku angielskim

            \vspace{2cm}
            \setlength{\columnsep}{-200pt}
            \begin{multicols}{2}

               \text{}

            \columnbreak
            \large AUTOR:


            Michał Żarejko


            \vspace{1cm}
            PROWADZĄCY PRACĘ:


            dr. mgr. Paweł Zyblewski

        \end{multicols}
   \vspace{3cm}

-----------------------------------------------------------------------------------------------


   Wrocław  2021

   \end{center}

\end{titlepage}

%\shipout\null

\newgeometry{top=2.5cm,bottom=2.5cm,right=2.5cm,left=2.5cm}
\tableofcontents{}
\newpage

\setlength{\parindent}{0.5em}
\setlength{\parskip}{0.5em}
\renewcommand{\baselinestretch}{1.5}



\chapter{Wstęp}

Uczenie maszynowe jest zagadnieniem rozwijanym od dłuższego czasu.
Już w latach 40 powstawały książki oraz programy powiązane ze sztuczną inteligencją \cite{hist AI}. 
Między innymi w 1940 roku Donald Hebb stworzył podstawy teoretyczne wykorzystane w 
późniejszych sieciach neuronowych \cite{hist AI}. Po mimo, wielu wczesnych pomysłów omawiany dział
nauki,
dopiero od niedawna zaczął osiągać wielkie sukcesy. Wynika to z faktu, że wiele algorytmów
potrzebuje
dużej mocy obliczeniowej  
i dopiero nowoczesny sprzęt był w stanie spełnić takie wymagania \cite{hist AI}.

Dzisiaj można wymienić wiele rozwijanych narzędzi związanych z tym tematem.
Między innymi asystenci głosowi, tłumacze językowe, modele wyświetlające elementy na
stronach internetowych, gry wideo lub inteligentne samochody.
Dodatkowo sztuczna inteligencja jest mocno wykorzystywana w firmach,
na halach produkcyjnych, w transporcie, medycynie albo cyberbezpieczeństwie. 


Pomimo tylu możliwości i zastosowań AI zyskuje aktualnie największą 
popularność
medialną przez gry rywalizacyjne, gdzie głównym zadaniem jest pokazanie przewagi 
algorytmów względem
ludzi. Dzisiaj można wymienić wiele takich wydarzeń, gdzie 
mistrzowie świata w danej grze, przegrywali z modelami rozpoznawania. 


Między innymi w 2016 roku zorganizowano
mecz między Fan Hui, mistrzem Europy w chińskiej grze $\text{Go}$ oraz algorytmem
$\text{AlphaGo}$ \cite{Go}.
Model utworzony przez zespół $\text{DeepMind}$ osiągnął duży sukces przez wygraną z przeciwnikiem.
Do tej pory gra była uważana powszechnie za skomplikowaną i trudną do rozwiązania.

W 2019 roku utworzono model zwany $\text{OpenAI Five}$ czyli pierwsze na świecie AI, które pokonało
zespół Team OG w rywalizacji $\text{e-sport}$ w grze $\text{Dota 2}$ \cite{Dota2}. Wyzwanie polegało
na rywalizacji 5-osobowych
zespołów.
Wydarzenie było mocno omawiane w mediach z powodu pierwszego takiego osiągnięcia w tej dziedzinie
sportu. 
Dodatkowo Dota 2 była bardzo skomplikowanym środowiskiem.
Przykładowo gra $\text{Go}$ rozwiązana parę lat wcześniej, zawierała 150 możliwych ruchów na turę, 
$\text{Dota 2}$
mogła posiadać ich 20 000 w niecałą godzinę \cite{Dota2}.


Istnieje wiele takich wydarzeń. Pokazują one, że w dzisiejszych 
czasach sztuczna inteligencja może przewyższać myśleniem strategicznym człowieka. Dodatkowo 
udowadniają one, że
temat AI jest dalej rozwijany i zyskuje coraz większe zainteresowanie.   

\section{Geneza tematu pracy}

Często w tworzeniu programów sztucznej inteligencji dużym 
wyzwaniem jest poziom skomplikowania gry. 
Zależy to między innymi od typu środowiska np. deterministycznego lub stochastyczne, 
od poziomu dynamiki gry lub od tego, czy przestrzeń wymiarowa jest
dyskretna, lub nieskończona.
Aktualnie jednak jednym z większych problemów takich programów jest niedostateczny
zakres dostępnych informacji o środowisku \cite{iig}.

Sztuczna inteligencja, aby zwyciężać,
musi nauczyć się grać, więc potrzebuje dużej ilości danych wejściowych, które są 
rozróżnialne. Przykładem gry, która jest pozbawiona tego problemu, są szachy. 
AI wykonuje ruchy, bazując na informacjach jak np. ułożenie pionków
w danej turze. Widoczna zmiana stanu środowiska przeciwnika jest
zauważalna przez gracza, przez co może on łatwiej powiązać
obserwacje z wykonywanymi akcjami. 

Przykładem gry ciężkiej w uczeniu AI w której, występuje niepełny zestaw informacji, 
jest $\text{Poker Texas Hold'em}$, pomimo wiedzy o kartach w ręce i na stole, gracz nie posiada
wiedzy o 
kartach przeciwników. W takim przypadku dwa pozornie identyczne stany środowiska w
rzeczywistości mogą się różnić. Z powodu takich cech większość popularnych algorytmów jak 
DQN (\emph{Deep Q Learning}), Actor-Critic lub AlphaZero staje się bezużyteczna i nie daje dobrych rezultatów.

W niniejszej pracy przedstawiono sposób możliwego rozwiązania takiego problemu przy pomocy 
algorytmu Deep CFR \cite{DCFR}. Jest to popularna metoda do tworzenia modeli
rozpoznawania w grach karcianych.

\section{Cel i zakres pracy}

Głównym celem pracy jest implementacji algorytmu Deep CFR, który stworzy 5 modeli
rozpoznawania w grze HULH ($\text{\emph{Heads Up Limit Texas 
Poker Hold'em}}$). Jest to popularna wersja rozgrywki 2-osobowej, gdzie uczestnicy nie mogą wybrać
samodzielnie kwoty podbicia stawki. Jest ona ograniczona przez ustaloną wartość. Takie środowisko
minimalizuje możliwe ruchy do 3 akcji, co czyni go prostszą bazą do uczenia maszynowego. Uzyskane modele zostaną następnie 
wykorzystane do stworzenia
rozgrywek składających się na wszystkie kombinacje dwóch modeli, gdzie każda gra zostanie 
powtórzona 200 razy. Drugim etapem
będzie obliczenie średniej puli wygrywanej i przegrywanej przez każdy model wraz z rozkładem
wykonywanych ruchów.
Taki proces pozwoli określić, który model osiąga najlepsze wyniki i jakiej strategii używa do gry.


Praca została podzielona w tym celu na rozdziały opisujące każdy z etapów projektu.
Następny rozdział jest wstępem teoretycznym do implementacji programu. Opisuje on 
możliwe rozwiązania problemu, analizę gry Poker Texas Hold'em oraz wymaganą teorię do zrozumienia
Deep CFR.
Trzecia część przedstawia implementacje programu wraz z użytymi technologiami. Końcowe rozdziały 
omawiają wyniki uczenia, rezultaty
rozgrywek modeli oraz podsumowanie pracy.

\chapter{Analiza istniejących rozwiązań}

W ciągu ostatnich 15 lat powstało wiele algorytmów rozwiązujących różne wersje gry Poker.
Między innymi CFR (\emph{Counterfactual Regret Minimization}) \cite{CFR}, XFP (\emph{Extensive-Form
Fictitious Play}) \cite{XFP} lub
NFSP (\emph{Neural Fictitious Self-Play}) \cite{NFSP}.
Pierwszy z wymienionych, CFR powstał w 2007 roku. Był 
pomyślną prób rozwiązania abstrakcyjnego środowiska Poker Texas Hold'em \cite{CFR}. 
Na jego podstawie utworzono wiele nowoczesnych
algorytmów, które dają szanse rozwiązać takie gry jak HULH \cite{CFR}.


Z wymienionych metod zaimplementowanym rozwiązaniem w niniejszej pracy jest CFR 
rozszerzony o sieci neuronowe,
czyli Deep CFR z grą HULH. Pozwala on na szybsze trenowanie modeli w środowisku typu zero-sum,
dodatkowo lepiej rozwiązuje gry o dużych rozmiarach \cite{DCFR}.

W tym rozdziale zostaną przedstawione profesjonalne sposoby wyboru strategii w grze Poker Texas Hold'em. 
Określą one cechy, jakimi powinien charakteryzować się prawidłowo utworzony model rozpoznawania.
Następnie rozdział przedstawi istniejące sztuczne inteligencje, które wykorzystały metodę CFR do
zwyciężania z profesjonalnymi graczami.
Ostatnim krokiem jest przedstawienie zbioru informacji wymaganych 
do zrozumienia implementowanego algorytmu.

\section{Analiza Texas hold'em Poker}

Jest to jedna z najpopularniejszych gier rywalizacyjnych w kasynach, dodatkowo jest to
dominująca gra hazardowa. Można ją scharakteryzować brakiem stanów deterministycznych oraz częściową 
obserwowalnością, tab. 2.1 \cite{poker}.  


Przez takie cechy gra była od zawsze tematem sporów, czy na jej wynik ma większy wpływ
losowość, czy umiejętności. Jak wynika z badań dużym aspektem pomagającym w osiągnięciu zwycięstwa,
jest panowanie nad emocjami, dokładna analiza stanu gry oraz umiejętność opóźnienia natychmiastowej
nagrody \cite{theory poker}.

\begin{table}[h!]
\centering
\caption{Charakterystyka gier}
\begin{tabular}{|c|c|c|c| }
   \hline
    & \makecell{środowisko \\ deterministyczne} & \makecell{środowisko \\ niedeterministyczne} \\
    \hline
   \makecell{pełny zestaw \\ informacji} &  \makecell{szachy \\ Go } & \makecell{Monopoly \\ Tetris }\\ 
   \hline
   \makecell{niepełny zestaw \\ informacji} &  \makecell{Saper \\ Mahjong } & \makecell{Poker \\
   Makao}  \\  
   \hline
\end{tabular}
\end{table}

\vspace{6cm}
Dodatkowo ważnym elementem jest obserwacja gry oraz wybór prawidłowej strategii.
Można ja wybrać na bazie dostępnych kart i dotychczasowego zachowania oponenta.

Kolejną ważną zasadą jest obserwacja przeciwnika oraz zapamiętywanie jego
poprzednich akcji. Przez to można określić, czy gra on agresywnie, czy pasywnie i dobrać do 
niego odpowiednią strategię. W tym celu dokonuje się klasyfikacji przeciwników na 
bazie częstotliwości wykonywanych ruchów \cite{class}.

Każdego gracza można podzielić na cztery grupy, Loose Aggressive, Losse Passive, Tight
Passive oraz Tight Aggressive \cite{class}. Prawidłowe rozpoznanie danego stylu gry
może zadecydować o wyborze prawidłowej strategii i zwycięstwie. Poniżej opisano każdy z nich oraz
porównano je z rys. 2.2, który pokazuje, w jakim stopniu profesjonalny gracz powinien używać każdego
z nich \cite{class}. 

\subsubsection{Loose Passive}

Osoba, która bardzo często wchodzi do gry niezależnie czy posiadane karty dają jej
wysokie szanse na wygraną. 
Ten typ gry nie jest dobrą strategią, ponieważ można łatwo się do niego 
dostosować przez używanie tylko mocnych kart \cite{class}.

\subsubsection{Loose Aggressive}

Osobę, która często przebija stawkę, zaczynając od rundy \emph{pre-flop} pod
warunkiem, że ma dobre startowe karty \cite{class}. Strategia ma stworzyć przekonanie wśród
oponentów, że gracz ma bardzo duże szanse wygranej od samego początku. 
Okazuje się ona jednak nieefektywna, jeśli przeciwnicy nie pasują w początkowych etapach gry
\cite{class}. 

\subsubsection{Tight Passive}

Uczestnik gry wchodzi tylko z dobrymi kartami, wykonując często akcję \emph{call}. 
Ostatecznie pasuje przy spotkaniu z graczem agresywnym. Taka osoba gra bardzo dokładnie tak, aby
mało ryzykować, przez co często traci wiele okazji, kiedy
mogła, by wygrać \cite{class}. 

\subsubsection{Tight Aggressive}

Grają podobnie do typu \emph{Tight Passive} w początkowych etapach gry, a następnie zmieniają swój
styl na bardziej agresywny \cite{class}. Rys. 2.1 pokazuje, że jest to najlepsza wersja strategii, jaką można
grać, dlatego często jest ona wybierana przez profesjonalnych graczy.

\vspace{1cm}


\begin{figure}[h!]
            \center
           \includegraphics[width=0.5\textwidth]{./img/class.pdf}
           \caption{Podział graczy \cite{class}.}
\end{figure}


Jak wynika z powyższych kategorii, gra Poker Texas Hold'em zawiera wiele elementów niezwiązanych z
losowością, gdzie obserwacje i dobieranie odpowiedniej strategi do typu gracza pełni kluczową
funkcję. Korzystając z tych informacji, będzie można określić poziom  zaawansowania utworzonych modeli, które
są opisane w rozdziale 4.

\section{Uczenie przez wzmacnianie}

Jest wiele sposobów na stworzenie sztucznych inteligencji w grach. Między innymi można użyć technik 
uczenia
nadzorowanego pod warunkiem, jeśli przygotuje się odpowiednie zbiory danych. W pracy jednak
zdecydowano się na stworzenie modelu, korzystając z uczenia przez wzmacnianie, czyli rozwiązania gdzie
AI nie potrzebuje wstępnej bazy uczącej. Wynika to z faktu, że jest mało publicznych zapisów
gry Poker Texas Hold'em z profesjonalnymi graczami, które mogłyby posłużyć jako zbiory uczące.
Algorytmy należące do
wybranego działu powinny być w stanie polepszać swoje wyniki na podstawie interakcji ze
środowiskiem bez korzystania z zewnętrznych materiałów.  


Wiele istniejących metod należących do wybranej techniki zakłada, że środowisko można opisać
przez model
matematyczny
MDP (\emph{Markov Decision Process}). 
Określa ona sekwencyjnie podejmowane decyzje w niepewnym środowisku \cite{mdp}. W każdym z nowych
stanów, w
jakich znajduje się agent (uczące się AI), wykonuje on pojedynczą akcję, zyskując od środowiska informacje o nowym stanie
oraz nagrodzie. Elementem wyjściowym zasady powinien być
zbiór strategii w postaci modelu, rys. 2.2.


\begin{figure}[th!]
            \center
           \includegraphics[width=0.5\textwidth]{./img/MDP.png}
           \caption{Schemat interakcji ze środowiskiem \cite{mdp}.}
\end{figure}


MDP może opisywać jedynie środowiska z pełnym zakresem informacji, w przypadku algorytmu będącego
tematem pracy, głównym zadaniem jest rozwiązanie środowiska z niepełnym zestawem danych. Wtedy 
należy rozpatrywać zasadę Partially Observable Markov Process, POMDP \cite{mdp}.


\vspace{2cm}
W przeciwieństwie do poprzedniej zasady tutaj agent nie zna aktualnego stanu, w którym
się znajduje \cite{mdp}. Przez takie okoliczności musi połączyć zależnością wykonywane akcie i
obserwacje, a nie stany. Większość gier karcianych można zakwalifikować do tego typu problemów
\cite{mdp}.

\section{Teoria Gier}

Aby zrozumieć działanie algorytmów CFR, Deep CFR, MCCFR itd. należy zapoznać się
z podstawami działu matematyki o nazwie Teoria Gier. Bada on optymalne zachowanie w środowiskach, gdzie
występuje konflikt \cite{gt}. W przypadku gry Poker Texas
Hold'em zostaną wytłumaczone terminy Równowaga Nasha oraz
postać ekstensywna. Są one obowiązkowe do zrozumienia algorytmu Deep CFR.

\subsubsection{Gra w postaci ekstensywnej}

Gry w
formie ekstensywnej można przedstawić jako drzewo decyzyjne, gdzie każdy węzeł rozgałęzia się na
możliwe akcje oraz identyfikuje aktualny stan gracza przez zestaw informacji.
Stany końcowe drzewa określają zysk lub stratę nagrody wybranego gracza \cite{gt}.
Jest to sposób
na uproszczenie opisu gry.

Na rys. 2.4 przedstawiono przykład gry 'Papier-Kamień-Nożyce' w formie ekstensywnej, gdzie gracze P1 i
P2 eksplorują 3 akcje w swoich węzłach. 
Każdy z nich uzyskuje wyniki danych ścieżek oraz zapamiętuje dotychczasową historię, co może zostać potem
wykorzystane do znalezienia najbardziej opłacalnych strategii.

\begin{figure}[th!]
            \center
           \includegraphics[width=0.9\textwidth]{./img/drawing1.pdf}
           \caption{przykład gry w postaci ekstensywnej.}
\end{figure}

\vspace{5cm}
\subsubsection{Równowaga Nasha}

W grach to twierdzenie określa perfekcyjny stan wyboru akcji, gdzie wszyscy gracze wykorzystują najlepszy
zestaw strategii, którego zmiana przyniesie tylko straty. Oznacza to, że nie jest możliwa
zmiana ruchów oraz zwiększenie uzyskanej nagrody \cite{gt}. 

Dobrym przykładem prezentującym taki stan jest "Dylemat Więźnia" \cite{rn}. To środowisko zawiera 
dwóch przestępców, którzy są przesłuchiwani w odseparowanych pomieszczeniach. Każdy z nich ma dwie
opcje, przyznać się do zarzutów lub tego nie robić. Każda z kombinacji akcji uczestników jest
zaprezentowana w tab. 2.2, gdzie wartości określają lata spędzone w więzieniu po danym ruchu.
Posługując się Równowagą Nasha, można stwierdzić, że najlepszą opcją dla
obu uczestników będzie przyznawanie się za każdym razem \cite{rn}. Wynika to z faktu, że wyniki
przegranej są tam małe wraz z brakiem ryzykowania porażką, czyli 5 latami więzienia. 

Głównym
zadaniem większości algorytmów gier karcianych bazujących na metodzie CFR jest znalezienie
takiego stanu. Deep CFR nie odnajduje go, ale odkrywa zbiory strategii, które są bliskie Równowadze
Nasha \cite{DCFR}.

\vspace{1cm}
\begin{table}[h!]
   \centering
\caption{Wyniki akcji środowiska "Dylemat więźnia".}
\begin{tabular}{|c|c|c|}
   \hline
   & \makecell{przyznanie się \\ więźnia A} & \makecell{więzień A \\ kłamie} \\ 
   \hline
   \makecell{przyznanie się \\ więźnia B} & \diagbox[innerwidth=3cm]{1}{1} & \diagbox[innerwidth=3cm]{0.5}{5} \\
   \hline
   \makecell{więzień B \\ kłamie} & \diagbox[innerwidth=3cm]{5}{0.5} & \diagbox[innerwidth=3cm]{0}{0} \\
   \hline
\end{tabular}

\end{table}



\section{Historia modeli Texas Hold'em Poker}

Bazując na Teorii Gier oraz innych twierdzeniach powstało wiele rozwiązań różnych wersji gry Poker.
Pierwsze dokumenty naukowe omawiały bardzo proste
środowiska jak Poker Kuhn \cite{DCFR}.
Dopiero w 2015 roku utworzono znaną sztuczną inteligencję $"\text{Cepheus}"$ rozwiązującą problem
HULH \cite{cepheus}. Było to pierwsze takie osiągnięcie w historii. Kolejnym etapem były prace nad
algorytmem mogącym rozwiązać problem gry HUNH (\emph{Heads Up No-limit Texas Hold'em}).
Powstały model w 2017 roku nazwano "DeepStack" \cite{ds}. Mieszał on sieci neuronowe z 
technikami algorytmu CFR. W podobnym czasie utworzono kolejne, tym razem najbardziej zaawansowane AI
w historii, Libratus \cite{libratus}.


Pomimo takich osiągnięć utworzone modele potrafią grać tylko
w środowiskach 
składających się maksymalnie z 2 osób typu zero-sum \cite{libratus} \cite{cepheus} \cite{ds}.  
Wynika to z poziomu skomplikowania gier częściowo-obserwowalnych.
Sposoby na jego rozwiązanie zaczęły powstawać od niedawna, a pierwsze
dwa duże osiągnięcia w grze HUNH miały miejsce dopiero w 2017 roku. Dodatkowo wszystkie wymienione
modele bazują na metodzie CFR lub na jej nowszych wersjach.


\subsubsection{Cepheus}

AI powstałe w celu wygrywania w grach HULH. Był to pierwszy model rozwiązujący dużą wersję gry Texas
Poker Hold'em w historii \cite{cepheus}.  Wykorzystał on nowszą wersję techniki CFR, którą nazwano CFR+
\cite{cepheus}. W wyniku dwóch miesięcy nauki i testów nowa metoda zbiegała się znacznie szybciej do
Równowagi Nasha niż bazowy CFR \cite{cepheus}. Powstały model jest udostępniony publiczne, każdy
może go przetestować.    

\subsubsection{DeepStack}

Model DeepStack rozwiązał HUNH przez połączenie metody CFR, sieci neuronowych wraz z dodatkowymi
elementami. W rezultacie AI zaczęło osiągać bardzo dobre wyniki.
Przetestowano go na 33 profesjonalnych graczach w wielu iteracjach gry. Model w większości
przypadków wygrał \cite{ds}. Była to pierwsza wygrana AI z człowiekiem w normalnej 
wersji gry Poker Texas Hold'em z taką częstotliwością.

\subsubsection{Libratus}

Najbardziej zaawansowana sztuczna inteligencja, która jest wykorzystywana w grach HUNH. Jak wynika z
testów wygrywa znacznie częściej niż DeepStack z profesjonalnymi graczami \cite{libratus}.
Przetestowano go z najlepszymi graczami na świecie, Dong Kim, Dan McAulay, Jimmy Chou 
i Jason Les \cite{libratus}. AI wygrało z nimi, z ogromną przewagą.




\section{Counterfactual Regret Minimization}

\subsection{Regret Matching}

Jest to nieodłączna metoda uczenia AI w grach karcianych. Polega ona na liczeniu najlepszej 
strategii pod warunkiem, że znany jest wektor żalu w węźle.
Taki wektor opisuje się jako tablicę wag o długości równej liczbie możliwych akcji gracza. Każda z 
tych wag opisuje, jak dużym błędem będzie niewykonania danego ruchu.


Poniżej przedstawiono wzór wynikający z tej metody, gdzie  
$R^{T}(a)$ jest omawianym wektorem \cite{CFR}. 
Następnie, aby uzyskać nową strategię, usuwa się wartości ujemne, czyli takie, których gracz nie
żałował (formuła nr 2.2).
Potem sprawdzana się, czy ich suma jest większa od zera w celu wybrania odpowiedniego wzoru, formuła
2.1.
W zależności od tego warunku otrzymuje się określony rozkład prawdopodobieństwa wykonania każdej z
akcji.


\begin{equation}
p^{t}_{i}\left( a \right) = \left\{ \begin{array}{ll}
      \frac{R^{T, \text{+}}\left(a\right)}{ \Sigma_{a' \in A} R^{T,\text{+}}\left(a'\right)} &
      \mbox{if $\Sigma_{a' \in A} R^{T,\text{+}}\left(a'\right) >
      0$};\\
      \frac{1}{|A|} & \mbox{$otherwise$}.\end{array} \right. \ 
\end{equation}

\vspace{1cm}
\begin{equation}
   R^{t,\text{+}}(a) = max(R^t(a),0)
\end{equation}


Proces ten jest powtarzany wielokrotnie, tak, aby przy każdej iteracji rozkłady prawdopodobieństwa
ruchów były stopniowo
poprawiane.

W przypadku algorytmu Deep CFR zachodzi modyfikacja formuły nr 2.1. Strategia jest liczona na
dodatnich wartościach żalu podzielonego przez  prawdopodobieństwo dostania się do 
tego stanu $D^{T} (I, a)$ \cite{DCFR}.
Jeśli suma jest ujemna, to zostaje wybrana akcja z najwyższą wartością $D^{T}(I, a)$ \cite{DCFR}.



\begin{equation}
\sigma^{t+1}_{i}\left(I, a \right) = \left\{ \begin{array}{ll}
      \frac{D^{T, \text{+}}\left(a\right)}{ \Sigma_{a' \in A} D^{T,\text{+}}\left(a'\right)} &
      \mbox{if $\Sigma_{a' \in A} D^{T,\text{+}}\left(a'\right) >
      0$};\\
      \text{argmax}(D^{T} (I, a)) & \mbox{$otherwise$}.\end{array} \right. \ 
\end{equation}



\subsection{Counterfactual Regret}

Algorytm CFR do znanych wcześniej metod dodał termin 'Immediate Counterfactual Regret' oznaczany przez $R^{T}_{i,
imm} (I)$, 
czyli żal przydzielony do węzła I.
Do obliczenia takiego parametru została zdefiniowana wartość "counterfactual utility" $u_{i}(\sigma,
I)$. Oznacza ona przewidywany wynik nagrody dla stanu I gdzie wszyscy gracze używają strategi
$\sigma$ \cite{CFR}. Dodatkowo $\pi^{\sigma} (h, h')$ oznacza prawdopodobieństwo dostania się z historii h do 
nowego stanu h' przy strategii $\sigma$ \cite{CFR}.

\begin{equation}
   u_{i} (\sigma, I) = \frac{\sum_{h \in I, h' \in Z} \pi^{\sigma}_{-i} (h) \pi^{\sigma} (h,
   h') u_{i}(h')}{\pi_{-i}^{\sigma}(I)}
\end{equation}

Na podstawie równania 2.5 można wyliczyć końcową wartość żalu w algorytmie CFR.

\begin{equation}
   R^{T}_{i,\text{imm}} (I, a) = \frac{1}{T} \sum^{T}_{t=1} \pi^{\sigma^{t}}_{-i} (I)
   (u_{i}(\sigma^{t}|_{I \rightarrow a}, I) - u_{i}(\sigma^{t}, I))
\end{equation}


Powyższe 2 równania można doprowadzić do formuły nr 2.6. Wartość $\pi^{\sigma} (h,h')$ została
zastąpiona przez liczbę 1, ponieważ
CFR zakłada, że dla $u_{i}(\sigma^{t}|_{I \rightarrow a}, I)$, gracz wykonuje zawsze akcję \emph{a} 
z całkowitą pewnością \cite{CFR}.



\begin{equation}
   R^{T}_{i,\text{imm}} (I, a) = \frac{1}{T} \sum^{T}_{t=1}
   \pi_{-i}^{\sigma}(h)\sum_{h \in I, h' \in Z}(1*u_{i}(h') - \pi^{\sigma}(h,h')u_{i}(h'))
\end{equation}


\vspace{0.5cm}
Po uzyskaniu $R^{T}_{i,\text{imm}} (I, a)$ można wykorzystać metodę "Regret Matchning" i
zaktualizować strategie.
Poniżej przedstawiono przykład obliczeń pojedyńczego węzła oraz wyniki 
dla gry "Papier-Kamień-Nożyce" przy ustawionych
nagrodach i karach w stanach końcowych jak na rys. 2.4, 2.5, 2.6.

\begin{center}
$$
   \pi^{\sigma}(h,h')u_{i}(h') =  (\frac{1}{3} \cdot 0) + (\frac{1}{3} \cdot -1) +
   (\frac{1}{3} \cdot 2) = \frac{1}{3}
$$

$$
T*R^{T}_{i,\text{imm}} (I, a) =
((0-\frac{1}{3}), (-1-\frac{1}{3}), (2-\frac{1}{3})) \cdot \frac{1}{3}= (-\frac{1}{9},
-\frac{4}{9}, \frac{5}{9}) 
$$

$$
\sigma^{t+1}_{i}\left(I, a \right) = (0, 0, 1)
$$
\end{center}
\vspace{0.5cm}


\begin{figure}[th!]
            \center
           \includegraphics[width=1\textwidth]{./img/drawing2.pdf}
           \caption{Przykład 'counterfactual utility'.}
\end{figure}

Na podstawie rys. 2.4 widać, że gracz P2 będzie miał stan o najwyższej wartości "counterfactual utility" w węźle $u_{i}
(\sigma^{t}, I)=\frac{1}{9}$, a najniższej dla $u_{i}(\sigma^{t}, I)=-\frac{1}{9}$.


\begin{figure}[th!]
            \center
           \includegraphics[width=1\textwidth]{./img/drawing3.pdf}
           \caption{Przykład 'Immediate Counterfactual Regret'.}
\end{figure}

\vspace{0.5cm}
\vspace{10cm}

Powyżej zaprezentowano wektory $R^{T}_{i,\text{imm}} (I, a)$.
Gracz P1 grając, najbardziej będze żałował nie wykonania akcji \emph{Kamień}, a najmniej
\emph{Papier}.

\vspace{0.5cm}
\begin{figure}[th!]
            \center
           \includegraphics[width=1\textwidth]{./img/drawing4.pdf}
           \caption{Przykład strategii.}
\end{figure}
 
Rys. 2.6 przedstawia wektory określające jakimi rozkładami akcji powinni się kierować gracze, aby
osiągnąć najlepsze wyniki. Są to wektory wyliczone po pierwszej iteracji, w praktyce eksploracja
drzewa i powyższe obliczenia są powtarzane wielokrotnie. Końcowym etapem algorytmu CFR jest
policzenie średniej strategii, która ma reprezentować Równowagę Nasha \cite{CFR}.





\section{Monte Carlo Conterfactual Regret Minimization}

Algorytm CFR eksploruje całe drzewa decyzyjnego w jednej iteracji, co tworzy wymagania na
dużą moc obliczeniową i długi czas uczenia. Dla małych gier takie rozwiązanie jest akceptowalne, ale w przypadku 
większych środowisk jest nieefektywne.
Spowodowało to powstanie nowszej wersji algorytmu CFR, MCCFR (\emph{Monte Carlo Conterfactual Regret
Minimization}) , 
który w każdą iterację eksploruje tylko część drzewa \cite{mccfr}.

Metodę można podzielić na dwie odmiany, Outcome-Sampling oraz External-Sampling \cite{mccfr}.

W pracy zostanie przedstawiony sposób MCCFR ES (\emph{Monte Carlo Conterfactual Regret
Minimization External Sampling}), ponieważ taki został zaimplementowany w algorytmie
Deep CFR.
MCCFR ES przed eksploracją drzewa wybiera kolejno spośród graczy jednego uczestnika, którego 
oznacza
się jako "traverser" \cite{DCFR}.
Eksploruje on wszystkie odpowiedzi ze swoich akcji w danym węźle. W międzyczasie inni
uczestnicy wykonują pojedynczy ruch na podstawie swojej najlepszej strategii \cite{mccfr}.  


Na rys. 2.7 przedstawiono przykład części drzewa HULH, 
gdzie gracz P2
został wybrany jako "traverser". 
Jak można zauważyć, tylko jego węzły rozgałęziają się na wszystkie możliwe ścieżki. 
Dodatkowo w drodze powrotnej obliczono $u_{i} (\sigma, I)$ dla wszystkich
stanów używając wzoru 
2.5.


\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{./img/tree.pdf}
  \caption{Część drzewa decyzyjnego MCCFR ES z graczem P2 jako "traverser".}
\end{figure}


\vspace{5cm}
Jest to przykład z jednej eksploracji gry, w praktyce powtarza się ten proces wielokrotnie, 
uzyskując różne wersje drzew.


Mając takie wyniki, można przystąpić do liczenia $R_{i,imm}^{T} (I)$ oraz poprawiania 
wartości przez "Regret Matching". MCCFR ES spełnia swoją funkcję, ale wymaga wielu iteracji, aby
uzyskać dobre wyniki. Z tego powodu w dalszym rozdziale zostanie przedstawiona metoda Deep CFR,
która przyspiesza proces uczenia przez użycie sieci neuronowych.

\section{Deep CFR}


Algorytm Deep CFR  opisany w \cite{DCFR} rozwija podstawową wersję metody CFR o sieci neuronowe.
Taka modyfikacja była wymagana, aby utworzyć AI, które może rozwiązać nie tylko proste gry,
ale też i duże jak HULH. Liczy on wektory w drzewie decyzyjnym
przez algorytm MCCFR ES. Dodatkowo Deep CFR zbiega się do Równowagi Nasha szybciej niż 
popularny algorytm NFSP z 2016 roku \cite{DCFR}.

Deep CFR wykorzystuje sieci neuronowe do przewidzenia wartości $D^{T}$(I, a) w podobnych
obserwacjach, potem na
podstawie predykcji liczy strategię ze wzoru nr 2.3.   
Następnie liczona jest zaktualizowana wersja wektora żalu, formuła nr 2.5.
Obliczone wyniki są dodawane do buforów  $($B_{1}$, $B_{2}$) \in B_{p}$, a strategia do zbioru $B_{s}$.


Po wielu iteracjach
rozpoczyna się nauka sieci z zebranej bazy. Poniżej przedstawiono dokładny opis Deep CFR.
\vspace{1cm}

\begin{algorithm}[H]
\SetKwInput{KwInput}{Wejście}                % Set the Input
\SetKwInput{KwOutput}{Wyjście}              % set the Output
\DontPrintSemicolon
  
\KwInput{$B_{p}$, $B_{s}$, $\theta_{s}$, $\theta_{p}$, $P$, $N$, $K$}
\KwOutput{$\theta_{s}$}
\For{n \textbf{in} $N$}{
   $h$, $t$ \leftarrow \text{Nowa gra}\\
   \For{$p$ \textbf{in} $P$}{
      \For{$k$ \textbf{in} $K$}{
         MCCFRES($\theta_{p}$, $\theta_{p-1}$, $p$, $t$, $h$, $B_{p}$, $B_{s}$)\\
      }
      $\theta_{p}$ \leftarrow \text{TRAIN($B_{p}$, $\theta_{p}$, $t$)} \Comment{loss:
         $\frac{1}{N_{batch}}$
      $\sum(y_{i}-\widehat{y_{i}})^2$$\cdot$$t_{i}$}\\
   }
}
$\theta_{s}$ \leftarrow \text{TRAIN($B_{s}$, $\theta_{s}$, $t$)} \Comment{loss:  $\frac{1}{N_{batch}}$$\sum(y_{i} -\widehat{y_{i}})^2$$\cdot$t_{i}$}\\
\textbf{return} $\theta_{s}$
 \caption{Deep CFR}
\end{algorithm}

\vspace{1cm}
Algorytm na wejściu dostaje argumenty przystosowane do gry 2-osobowej.
Pierwszymi elementami są bufory graczy ($B_{1}$, $B_{2}$) oraz kontener na 
strategie $B_{s}$.
Dodatkowo metoda potrzebuje listę uczestników \emph{P}, z których będzie wybierany cyklicznie gracza
eksplorującego drzewo decyzyjne.
Ostatnimi argumentami są iteracje gry \emph{N} oraz liczba cykli \emph{K} metody MCCFR ES.


W metodzie należy ustawić trzy pętle wraz z nową rundą, uzyskując początkową historię oraz
krok gry t. Kolejnym etapem jest
wykonanie funkcji \emph{MCCFRES}. Przyjmuje ona na wejściu sieci neuronowe obu graczy, 
listę uczestników p,
numer kroku t, historię rundy h oraz bufory. Po k 
powtórzeniach następuje uczenie sieci neuronowej wybranego wcześniej gracza eksplorującego drzewo. 

Model używa 
zmodyfikowanej wersji funkcji MSE (\emph{Mean Square Error}) do liczenia błędu predykcji. Każdy wynik $(y_{i} - \hat{y_{i}})^2$
mnożony jest przez krok $t_{i}$ w, którym uzyskano $y_{i}$ \cite{DCFR}.
Dodatkowo jak wynika z badań, algorytm osiąga lepsze wyniki
jeśli
sieci neuronowe są trenowane od początku \cite{DCFR}. Dlatego należy przed funkcją \emph{TRAIN}
wyczyścić model.

Po wszystkich powtórzeniach i zebraniu całej bazy bufora $B_{s}$, rozpoczyna się trenowanie sieci 
neuronowej $\theta_{s}$ w ten sam sposób jak inne modele. Elementem wyjściowym Deep CFR jest sieć 
$\theta_{s}$.


\vspace{1cm}
\begin{algorithm}[H]
\SetKwInput{KwInput}{Wejście}                % Set the Input
\DontPrintSemicolon
\SetKwFunction{FMain}{MCCFRES}
\SetKwProg{Fn}{Function}{:}{}
  
\Fn{\FMain{$\theta_{p}$, $\theta_{p-1}$, $p$, $t$, $h$, $B_{p}$, $B_{s}$}}{
   $t_{r}$ \leftarrow h \Comment{sprawdzenie czyja jest aktualnie tura}\\

   \uIf{$h$ jest stanem końcowym Z}{
      \textbf{return} $u_{p} (h)$
   }
   \uElseIf{$p$ = $t_{r}$}{
      $\hat{D}(I)$ \leftarrow \text{obliczenie wektora $\hat{D}(I)$ używając h}\\
      $\sigma (I)$ \leftarrow \text{Obliczenie strategi $\sigma_{t_{r}} (I)$, kożystając z $\hat{D}(I)$ i
         wzoru 1.1}\\

      \For{a \textbf{in} A(h)}{
         $u_{t_{r}}$(h) \leftarrow \FMain{$\theta_{p}$, $\theta_{p-1}$, $p$, $t+1$, $h+a$, $B_{p}$,$B_{s}$}\\
          
      }
      $u_{r_{r}}(\sigma, I)$ \leftarrow $\sum (u_{t_{r}}(h) \cdot \sigma(I))$\\
      $R^{T}_{t_{r}, imm}$(I) \leftarrow ($u_{t_{r}}(h) - u_{t_{r}}(\sigma, I)$)\\
      $B_{t_{r}}$ \leftarrow \text{dodanie próbki do bufora } $[R^{T}_{t_{r}, imm}$(I), h, t]$\\

   }
   \Else{
      $\hat{D}(I)$ \leftarrow \text{obliczenie wektora $\hat{D}(I)$ używając $\theta_{p}$ i
      stanu h}\\
      $\sigma (I)$ \leftarrow \text{Obliczenie strategi $\sigma_{p} (I)$, kożystając z $\hat{D}(I)$ i
         wzoru 1.1}\\
      $B_{s}$ \leftarrow \text{dodanie próbki do bufora } $[\sigma (I), h, t]$\\
      a \leftarrow $\sigma (I)$

      \textbf{return} \FMain{$\theta_{p}$, $\theta_{p-1}$, $p$, $t+1$, $h+a$, $B_{p}$,
         $B_{s}$}\\
   }
}
 \caption{Implementacja MCCFRES kożystając z sieci neuronowych}
\end{algorithm}

\vspace{1cm}

Implementacja MCCFR ES przedstawiona powyżej przy zadanych argumentach rozpoczyna się od
sprawdzenia, który gracz rozpoczyna daną turę. Na podstawie tej informacji będzie wykonywana
dalsza część algorytmu.


Pierwszym krokiem jest sprawdzenie, czy stan gry jest końcowym etapem. W przypadku prawdziwego 
warunku zwracana jest wygrana lub przegraną wartość stawki. 
Jeśli powyższy etap jest fałszywy, algorytm sprawdza, czy gracz jest oznaczony jako "traverser".
Wtedy program używając sieci neuronowej gracza,
otrzymuje wektor $\hat{D}(I)$ przez wprowadzenie do modelu informacji o widocznych kartach oraz
dotychczasowej historii gry. Korzystając ze wzoru 2.3, liczy strategie, odczytuje $u_{t_{r}}$ (h) 
wykonując rekurencję. Ostatnim krokiem jest wyliczenie wektora żalu i dodanie go do bufora.

Jeśli powyższy warunek był fałszywy, gracz liczy nowy rozkład
akcji i dodaje go do zbioru strategii. Następnie korzystając z tej sieci neuronowej i otrzymanej 
dystrybucji wykonuje nową akcję.


\section{Podsumowanie}

Środowiska z niepełnym zestawem informacji i brakiem deterministyczności są trudne do rozwiązania.
Algorytmy tworzące modele
dla takich gier są często skomplikowane i obciążające obliczeniowo. 
Przez takie cechy dopiero od niedawna zaczęły powstają algorytmy, zdolne pokonać ludzi w 
dużych grach karcianych jak "Cepheus", "DeepStack" lub "Libratus". 
Metody zdolne tworzyć takie AI dalej są rozwijane i aktualizowane z roku na rok.
W taki sposób w 2014 roku powstał CFR, potem MCCFR i po para latach zastąpiono je przez CFR+, aż 
zaczęto wykorzystywać sieci neuronowe, przez co powstał 
Deep CFR będący tematyką pracy. 

Rozdział dokładnie opisał działanie omawianego algorytmu
przez przedstawienie terminów należących do działu matematyki Teoria Gier. Między innymi
 pokazał, że opis środowiska przez drzewa decyzyjnych pozwala na wiele uproszczeń i możliwości
 śledzenia gry. Dodatkowo przedstawiono problem poszukiwania stanu Równowagi Nasha, którego
 znalezienie jest celem większości algorytmów sztucznej inteligencji gier karcianych.

Dobrze zaimplementowany algorytm Deep CFR przy prawidłowej parametryzacji i odpowiednio dużej
liczby iteracji powinien zbiec się do punktu bliskiego takiego stanu. Pomimo tak optymistycznych 
założeń utworzone AI ("Cepheus", "DeepStack", "Libratus") potrzebowały bardzo wielu iteracji do 
nauczenia się dobrej gry w HULH i HUNH.
Przykładem jest "Cepheus", który uczył się przez dwa miesiące, aby móc grać w HULH.

\chapter{Implementacja algorytmu} 

Program Deep CFR został napisany w niniejszej pracy, korzystając z narzędzi,
pozwalających na zredukowanie nadmiarowości kodu oraz na prostą implementację.
W tym rozdziale skupiono się na opisaniu wybranych technologii, parametrów oraz funkcji, 
które znalazły się w implementacji algorytmu.

Bazą do napisanego kodu jest język programowania, Python 3.8. Zawiera on wiele
technologii wspomagających uczenie maszynowe i rozległą społeczność wspierającą jego rozwój.
Poniżej 
wylistowano i opisano główne narzędzia użyte w pracy.

\paragraph{TensorFlow}

Jest to wysokopoziomowe API dostępne dla takich języków jak Python, JavaScript, C++ lub Java
\cite{tensorflow}.
Wykorzystuje je się głównie do zadań głębokiego uczenia maszynowego. Przez swoją prostotę, dostępność 
i dobrą
dokumentację stał się jednym z najpopularniejszych narzędzi wykorzystywanych do tworzenia
sztucznych inteligencji. Dodatkowo od niedawna biblioteki technologi Keras stały się częścią Tensorflow.
Daje to możliwości znacznego
zredukowania kodu przy prostych problemach, które często są rozwiązane przez funkcje w tym module. 


W przypadku niniejszej pracy głównie korzystano z funkcji zawartych w bibliotekach Keras. Wyjątkiem 
są nieliczne wiersze w kodzie gdzie np. było wymagane wykonanie obliczeń na tensorach.

\paragraph{Numpy}

Duża biblioteka do naukowych obliczeń na wielowymiarowych tablicach \cite{numpy}. Jest nieodłącznym
elementem przy pisaniu programów uczenia maszynowego, zwłaszcza jeśli korzysta się z 
bibliotek Tensorflow. Wynika to z faktu, że wiele funkcji tego API, jako argumenty przyjmuje
typy danych powiązane z Numpy \cite{tensorflow}.



\paragraph{Tqdm}
Małe narzędzie w języku Python pozwalające na wyświetlenie postępu procesów w działającym 
programie.
Przydatne w celach
testowych. W pracy zostało użyte do śledzenia iteracji drzew decyzyjnych 
algorytmu Deep CFR.

\paragraph{TensorBoard}
Moduł należący do API Tensorflow. Wizualizuje postępy uczenia sieci neuronownych oraz ich 
jakość przez przedstawienie odpowiednich wykresów. 


\paragraph{PyPokerEngine}

Biblioteka wspomagająca symulację gry Poker Texas Hold'em.
Użyto jej jako podstawę do napisania środowiska HULH do interakcji z 
metodą Deep CFR.

\section{Implementacja sieci neuronowych}

Algorytm Deep CFR do działania wymaga dwóch sieci neuronowych, jedna ma rozpoznawać strategie
$\sigma^{t}$, a
druga przypisana do określonego gracza przewiduje opłacalność akcji $D_{p}^{t}$. 
Dodatkowo każdy z tych elementów
jest trenowany na podstawie cyklicznie aktualizowanych buforów $B_{s}$ i $B_{p}$. W tym rozdziale zostanie
przedstawiona dokładna implementacja modeli, proces ich uczenia, struktura zbiorów danych oraz
budowa środowiska, z którym jest wykonywana interakcja. W tab. 3.1 i 3.2 zamieszczono podstawowe 
informacje
o parametrach sieci. Dalsza część rozdziału dokładnie opisuje dodatkowe elementy, ważne podczas
uczenia modelu.

\vspace{1cm}
\begin{table}[h!]
\centering
\caption{Podstawowe parametry sieci neuronowej $\theta_{s}$.}
\begin{tabular}{|c|c| }
   \hline
   parametr & użyte wartości \\
    \hline
   rozmiar danych wejściowych & (3, 52) \\
   \hline
   rozmiar danych wyjściowych & (None, 3) \\  
   \hline
   prędkość uczenia & 0.0001 \\
   \hline
   końcowa funkcja aktywacyjna & \emph{softmax} \\
   \hline
   rozmiar bufora & 600 000 \\
   \hline
   maksymalna liczba iteracji & 5 000 \\
   \hline
   rozmiar \emph{minibatch} &  500\\
   \hline
   parametr \emph{patience} &  40\\
   \hline
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Podstawowe parametry sieci neuronowej $\theta_{p}$.}
\begin{tabular}{|c|c| }
   \hline
   parametr & użyte wartości \\
    \hline
   rozmiar danych wejściowych & (3, 52) \\
   \hline
   rozmiar danych wyjściowych & (None, 3) \\  
   \hline
   prędkość uczenia & 0.0001 \\
   \hline
   końcowa funkcja aktywacyjna & \emph{linear} \\
   \hline
   rozmiar bufora & 300 000 \\
   \hline
   maksymalna liczba iteracji & 5 000 \\
   \hline
   rozmiar \emph{minibatch} &  500\\
   \hline
   parametr \emph{patience} &  40\\
   \hline
\end{tabular}
\end{table}

\vspace{5cm}
\subsection{Architektura modelu}

W algorytmie zaimplementowano trzy sieci neuronowe $\theta_{1}$, $\theta_{2}$, $\theta_{s}$ o nieskomplikowanej architekturze. Z tego powodu
wykorzystano bibliotekę Keras, która do takich przypadków jest dobrym rozwiązaniem. 
Dodatkowo
założono, że
wszystkie modele będą miały podobną budowę poza ostatnią warstwą z innymi funkcjami
aktywacyjnymi. Wynika to z faktu, że algorytm Deep CFR korzysta z sieci, które dostają dane wejściowe
o tej samej strukturze, ale zwracają $D_{p}^{t}$ lub $\sigma_{p}^{t}$.

Architektura sieci neuronowych składa się z 7 elementów, wejścia, wyjścia, bloku o nazwie
$\emph{Flatten}$ \cite{tensorflow}, 
trzech warstw ukrytych zakończonych normalizacją i wyjściem w postaci wektora o trzech polach.


Początkowo sieci dostają tablicę o wymiarach (3, 52), czyli kolumnę kart ze stołu, z ręki oraz
historię ostatniej rundy gry. Model w dalszych obliczeniach musi
przekształcić takie dane do formy jedno-wymiarowej (None, 156), co robi warstwa $\emph{Flatten}$.
Kolejne dwa elementy składają się z 512 wag, trzecia warstwa ukryta posiada ich 256.
Na trzech wymienionych elementach ustawiono funkcję \emph{Relu}.
Całość kończy się wyjściem wektora reprezentującego możliwe akcje gry HULH.
Dodatkowo dane są poddawane normalizacji przez warstwę o nazwie $\emph{BatchNormalization}$
\cite{tensorflow}.


Wyjście modeli, aby zwracało prawidłowe liczby, używa innej funkcji aktywacyjnej niż poprzednie
elementy. W przypadku predykcji
strategii $\sigma_{p}^{t}$ wybrano \emph{softmax}. 
Sieci neuronowe $\theta_{1}$, $\theta_{2}$ używają funkcji \emph{linear}.
Dokładna architektura sieci jest zaprezentowana na rys. 3.1.


\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{./img/nn.pdf}
  \caption{Architektura sieci neuronowych wykorzystana w programie.}
\end{figure}


W trakcie trenowania sieci korzystają z funkcji optymalizującej Adam.
Prędkość uczenia jest równa 0,0001, co spowoduje powolne uczenie, ale zminimalizuje szanse na
ominięcie minimum globalnego. 

Jak wynika z dokumentu prezentującego algorytm Deep CFR, uzyskuje on lepsze wyniki, jeśli sieci
neuronowe są uczone za każdym razem od początku przy losowo ustawionych zerach w wagach \cite{DCFR}. 
W tym celu ustawiono w każdej warstwie funkcję, która tworzy losowo wartości w przedziałach od
-0,005 do 0,005.

Ostatnim elementem sieci jest funkcja licząca błąd predykcji w trakcie uczenia. Jak wynika z 
opisu algorytmu Deep CFR, wymaga on zmodyfikowanej wersji MSE (\emph{Mean Square Error}).
Zostało to wykonane przy pomocy 
funkcji matematycznych na tensorach, jakie udostępnia \emp{Tensorflow} \cite{tensorflow}.

Wszystkie te elementy zostały uwzględnione w funkcjach klasy \emph{Poker\_network} przedstawionej na
rys. 3.4.


\subsection{Budowa zbiorów danych}

Jak wynika z algorytmu Deep CFR, w programie muszą być zawarte trzy bufory.
Maksymalna pojemność kontenerów $B_{1}$ i $B_{2}$ jest równa 300 000 próbek.
Bufor $B_{s}$ może posiadać ich 200 000. Każdy z dodawanych elementów do bufora składa się z
trzech połączonych wektorów o rozmiarach 52.

Zaimplementowano klasę $\emph{Memory}$ zarządzającą tymi buforami, które zachowują się jak 
kolejki \emph{deque}, w przypadku przepełnienia jest usuwany najstarszy wpis.

Mając uzupełnione dane w tablicach, program rozpoczyna przygotowanie zbiorów danych do uczenia
wybranych
sieci neuronowych. Każdy z buforów zostaje losowo przetasowany i podzielony na dwa podzbiory.
Pierwszy z nich to zbiór uczący zajmujący 80\% całej bazy. Wykorzystuje się go do poprawiania wag modelu sekwencyjnie.
Drugim zbiorem
są dane walidacyjne. Dokonanie takiego podziału było wymagane, aby przeciwdziałać stanowi
przetrenowania modelu. Zbiór walidacyjny jest nadzorowany i na jego podstawie można określić 
moment od, którego model przestaje dobrze działać. Schemat tego
podziału przedstawiono na rys. 3.2. 
Dodatkowo w trakcie nauki sieć neuronowa aktualizuje swoje wagi w każdym kroku przez użycie elementu o nazwie
\emph{minibatch}, będącym parametrem określającym
mały podzbiór bazy użyty do trenowania sieci w pojedynczym kroku. Jego liczebność jest równa 500
próbek.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{./img/bzd.pdf}
  \caption{Podział danych.}
\end{figure}


\subsection{Proces uczenia}

Algorytm Deep CFR do eksploracji wykorzystuje metodę MCCFR ES, która eksploruje w jednej iteracji
wiele razy drzewo dezyzyjne gromadząc przy tym próbki w buforach.
Po zakończeniu 
wszystkich powtórzeń zachodzi etap uczenia sieci neuronowych $\theta_{1}$, $\theta_{2}$
wykonując maksymalnie 5 000 aktualizacji wag.
Taki schemat działania jest wykonywany dla obu graczy i powtarza się wielokrotnie. Końcowym
zadaniem jest wytrenowanie sieci neuronowej $\theta_{s}$.  
Wszystkie te elementy zostały połączone przez klasę $\emph{Brain}$, agregującą obiekty
$\emph{Memory}$
oraz $\emph{Poker\_network}$, rys. 3.4. 

Dodatkowo w celu poprawienia wyników uczenia użyto funkcji \emp{EarlyStopping} \cite{tensorflow}.
Zatrzymuje ona iteracje modelu w przypadku kiedy błąd predykcji na zbiorze
walidacyjnym wzrośnie odpowiednio wysoko. Jest to sprawdzane na podstawie argumentu \emph{patience},
który określa próg, po którym nauka się kończy. Taka procedura została zastosowana, aby
zminimalizować szanse na przetrenowanie modeli, a wraz z tym ich gorszą jakość \cite{early}.
Rys. 3.3 prezentuje przykładowy punkt zakończenia nauki modelu.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.7\textwidth]{./img/early.pdf}
  \caption{Przykładowy punkt zatrzymania się uczenia po zastosowaniu \emph{EarlyStopping} \cite{early}.}
\end{figure}

\vspace{2cm}
Do samego trenowania sieci neuronowych wybrano urządzenie GPU GeForce GTX 1050 z 
racji uzyskiwania szybszych
rezultatów w przeciwieństwie do innej możliwości jaka jest wykorzystanie CPU.


\section{Implementacja środowiska}

Do symulacji gry HULH z którą model będzie się komunikował wykonując akcje, użyto blblioteki PyPokerEngine.
Klasa 
\emph{HULH\_Emulator} 
zaimplementowana w programie pozwala na utworzenie obiektu
zarządzającego taką grą. Przy tworzeniu obiektu zostają zdefiniowane 
nazwy graczy P1 i P2, które w dalszej części będą używane do śledzenia histori gry oraz wykonywania
wszelkich akcji graczy. 
W tabeli nr 3.3 wylistowano parametry ustawione w programie, związane z zasadami grą.


\begin{table}[h!]
\centering
\caption{Parametry gry HULH.}
\begin{tabular}{|c|c| }
   \hline
   parametr & wartość \\
    \hline
   ante & 0  \\ 
   \hline
   mała w ciemno & 5  \\  
   \hline
   duża w ciemno & 10 \\
   \hline
   liczba rund po których resetuje się środowisko & 1  \\
   \hline
   udział każdego z graczy (stock) & 80  \\
   \hline
   liczba graczy & 2 \\
   \hline
\end{tabular}
\end{table}



Zaimplementowane środowisko każdej wprowadzonej akcji zwraca dwa
elementy, obiekt \emph{State} określający stan gry oraz historię gry w formie słownika. 
Obie wartości są używane do
śledzenia stanu środowiska w drzewie decyzyjnym. Informacje jak karty na stole, historia gry lub 
wygrana gracza jest zarządzane przez obiekt.  

Sama gra działa w taki sposób przy zadanych parametrach aby każde podbicie stawki było wielkości
dużej w ciemno. Dzięki takiemu założeniu gra nie kończy się zbyt szybko 
podwarunkiem, że żaden z graczy nie wykona akcji \emph{fold}. Dodatkowo ustalono, że po każdej 
rundzie środowisko jest resetowane. Taki proces powoduje powstawanie mniejszych drzew
decyzyjne co pozwala na szybszą naukę modeli przez krutszy czas poświęcany na eksplorację gry przez
MCCFR ES. 


\section{Implementacja Deep CFR}

Klasa \emph{DCFR} zawiera implementację algorytmu Deep CFR. 
Przy tworzeniu obiektu powstają w
konstruktorze trzy elementy gracz, oponent oraz strategia $\sigma$ z klasy \emph{Brain}.
Do tego ustawiane jest środowisko \emph{HULH\_Emulator} przez referencje.

Cały proces powstawania modelu rozpoczyna się od funkcji \emph{iterate}, która wykonuje trzy pętle
\emph{for},
dla iteracji algorytmu po 50 razy oraz powtórzeń eksploracji drzewa dla każdego z gracza po 270 razy. W pierwszej z nich 
środowisko tworzy nową grę, w ostatniej pętli jest wykonywana funkcja \emph{\_\_traverse},
działająca wedłóg metody MCCFR ES. Dostaje ona argumenty \emph{state} czyli obiekt określający
stan gry, \emph{events} - dotychczasową historię oraz \emph{timestep}.
Ostatni argument \emph{verbose} jest opcialny i służy tylko do wizualizacji powstałego drzewa
decyzyjnego. Funckja działa po przez rekurencję co oznacza, że aktywuje samą siebie wielkrotnie co
pozwala na sprawdzenie każdej wersji gry.
Po zakończeniu działania MCCFR ES, rozpoczyna się uczenie sieci neuronowej $\theta_{p}$. I
powtażanie powyrzszego procesu. 
Ostatnim etapem jest trenowanie sieci $\theta_{s}$ oraz zapisanie jej do pliku. Aby spełnic warunek 
utworzenia pięciu modeli kolejno ustawiono w funkcji \emph{iterate} opciajlny argument
\emph{checkpoint}. Przyjmuje on bazowo wartość \emph{None}. Przy uruchamianiu programu zmieniono 
ją na liczbę 10 co oznacza, że co 10 iteracji będzie trenowana sieć $\theta_{s}$ z uzberanego bufora
i zapisywana do pliku. 

Po ustawieniu wszystich parametrów uruchomiono program, który wykonywał sie przez trzy dni.
Tab. 3.4 prezentuje podstawowe parametry zaimplementowanego algorytmu.


\begin{table}[h!]
\centering
\caption{Parametry algorytmu Deep CFR.}
\begin{tabular}{|c|c| }
   \hline
   parametr & wartość \\
    \hline
   liczba powtórzeń eksploracj drzewa & 270  \\ 
   \hline
   liczba iteracji algorytmu & 50  \\  
   \hline
   co ile iteracji wytrenować i zapisać model & 10 \\
   \hline
\end{tabular}
\end{table}



\section{Podsumowanie}


W tym rozdziale zaprezentowano implementację algorytmu wraz z użytymi technologiami. Program
następnie został uruchomiony  z zadanymi parametrani jak 50 iteracji po 270 eksploracji drzewa.
Wykonywał sie on przez okres trzech dni używając GPU. Postępy
tworzonych pięciu modeli były na bieżąco analizowane i zapisywane. Przedstawiono między innymi struktórę sieci
neuronowych, budowę zbioru danych, działanie środowiska HULH oraz funkcje związane z uczeniem
algorytmu. Pokazano cel użycia takich metod jak \emph{EarlyStopping} lub \emph{Flatten}. 

Następny
rozdział przedstawi zapisane wyniki ilustrujące jakość utworzonych modeli oraz sprawdzi, który z
nich będzie najlepiej grał w HULH.



\begin{figure}[!ht]
  \centering
  \hspace*{-1cm}   
  \includegraphics[width=1.1\textwidth]{./img/uml.pdf}
  \caption{Diagram UML projektu.}
\end{figure}





\chapter{Wyniki}

Rozdział przedstawia rezultaty powstałe po około czterech dniach nauki modelu, następnie
pezedstawiono wyniki
wszystkich kombinacji rzgrywek między modelami oraz średnie wartości wygrywanych pól.
Zostaną tutaj zaprezentowane różnice między utworzonymi AI wraz z ich przewidywaną jakością
korzystając z odpowiednich wykresów bazujących na danych pobranych z modułu \empg{TensorBoard}. 
Dodatkowo zostanie dokonana próba wyboru najlepszego modelu na podstawie
rozegranego turnieju oraz klasyfikacja jego stylu gry na
podstawie pojedyńczego zapisu rozgrywki z drugim najlepszym modelem.
Ostatnim etapem będze sprawdzenie o ile lepiej radzi sobie najlepsze AI z graczami wykonującymi
tylko ruchy w pełni losowe oraz takim, który gra całkowicie szczerze. 


\section{Proces uczenia modelów rozpoznawania}

Algorytm Deep CFR zdołał utworzyć pięć modeli rozpoznawania, gdzie każdy proces nauki
był
śledzony przez funkcje biblioteki \emph{TensorBoard}. Uzyskane dane następnie zostąły
użyte do utworzenia czytelnych wykresów ilustrujących zależność błędu predykcji od liczby
iteracji.

Pierwsze utworzone AI powstał po 10 iteracjach metody Deep CFR co zajeło około 5 godzin.
Następnie rozpoczęła się nauka sieci $\theta_{s}$ ze zbiorem danych $B_{s}$ zapełnionym przez około 300 000 próbek.
Można zaóważyć na rys. 4.1,
że wartość błędu po 40 krokach uczenia nie zmieniła się mocno dla obu zbiorów.
W przypadku danych walidacyjnych sieć neuronowa przy dokonywaniu predykcji powdowała duże oscylacje
wartości pozostając na poziomie około 2,2 co przyczyniło sie do zakończenia procesu przedwcześnie przez \emph{EarlyStopping}.
Przy takim okresie model zmniejszył błąd predykcji zbioru uczącego wynoszący początkowo 0,2 do wartości bliskiej 0.

Analizując taki wykres można stwierdzić, że model nauczył się z zebranych danych zależności między
wprowadzanymi obserwacjami, a zwracanymi akcjami. Dodatkowo rys. 4.6 pokazuje, że takie AI gra lepiej
względem wielu utworzonych poźniej modeli.


\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{./img/model1.pdf}
\caption{Wyniki uczenia modelu nr 1.}
\end{figure}


\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{./img/model2.pdf}
\caption{Wyniki uczenia modelu nr 2.}
\end{figure}


\newpage
Po 20 powtórzeniach algorytmu Deep CFR, utworzono drugi model rozpoznawania. 
Nastapiło to po około 12 godzinach. Algorym do nauki użył prawie całego zapełnionego buforu $B_{s}$.
W zbiorze znajdowały się nowe próbki oraz te które zebrano przed nauką pierwszego AI.
Proces uczenia przebiegał początkowo w podobny sposób jak w poprzednim etapie. Model zaczął od błędu
predykcji 2,2 dla zbioru walidującego i błędu 0,2 dla buforu uczącego. Pierwszą różnicą względem 
poprzedniego etapu jest lepsza nauka na podstawie buforu walidacyjnego, po 10
powtórzeniach wartości zaczęły mocno
maleć aż do błędu predykcji wynoszącej około 1,4. Następnie pojawił się krótki wzrost wartości i
utrzymywanie się
na tym samym poziomie z lekkimi oscylacjami co spowodowało 
zakończenie procesu. Wykres 4.2 dobrze pokazuje cel używania biblioteki \emph{EarlyStopping}.
Prawdopodobnie przy większej liczbie iteracji model zaczą by uzyskiwać coraz gorsze wartości
predykcji zbioru walidacyjnego. Podsumowując model uzyskał lepsze wyniki względem etapu pierwszego
co może wynikać z większego zbioru danych. Po mimo takich zalet rys. 4.6 pokazuje, że utworzony
model przegrywa z AI numer 1 przy 400 potwórzonych grach.

Kolejne AI zostało wytrenowane po 30 iteracjach. Był to drugi dzień działania algorytmu
Deep
CFR. W tym momencie bufor $B_{s}$ był całkowicie zapełniony przez co
nowe próbki dodawane do zbioru usuwały najstarsze wpisy. Na tak dużej bazie model uczył sie 
przez około 100
powtórzeń. Nauka modelu na podstawie zbioru uczącego była podobna jak w poprzenich etapach. Główną
różnicą jest bufor walidacyjny, dla którego błąd początkowo wyniusł 3,5 i zmalał do 1,7. 

\vspace{0.5cm}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{./img/model3.pdf}
\caption{Wyniki uczenia modelu nr 3.}
\end{figure}


\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{./img/model4.pdf}
\caption{Wyniki uczenia modelu nr 4.}
\end{figure}

Model nr 4 powstał po 40 iteracjach. Wytrenował sieć neuronową po około 75 potwórzeniach gdzie 
wartości błedu predykcji prezentuja sie w podobny sposób jak w poprzednim etapie. Jedyną róznicą
jest gwałtowny spadek błędu predykcji w pierwszych 5 iteracjach.

Ostanie AI utworzono po 4 dniach. Rys. 4.5 pokazuje, że proces nauki przebiegał początkowo z 
niewielkim błędem predykcji i powoli sie zmniejszał do wartości 1,9 w przypadku zbioru walidacyjnego.
Drugi zbiór doprowadził do podobnych rezoltatów jak we wszystkich etapach.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{./img/model5.pdf}
\caption{Wyniki uczenia modelu nr 5.}
\end{figure}

Podsumowując powyżesze wyniki można stwierdzić, że zbierane dane w buforze $B_{s}$
przez Deep CFR są łatwe w znajdowaniu zależności. Główną przyczyną mogą być dane 
wejściowe i wyjściowe o niewielkim rozmiarze wraz z prostym środowiskiem.
Prawdopodobnie przy grach dłuższych niż jedna runda, gdzie gracz przegrywa dopiero jak straci 
wszystkie żetony, oraz przy większym zbiorze informacji wejściowych proces uczenia by nie 
przebiegał tak szybko. W takim przypadku model by musiał uwzględnić dodatkowe czynniki jak 
numer rundy, liczba pozostałej sumy żetonów gracza lub przewaga przeciwnika. 

\section{Wyniki rozgrywek modeli}


W celu przetestowania jakości utworzonych modeli przeprowadzono 10 
rozgrywek gdzie każda z nich to
pojedyncza kombinacja dwóch AI. Każdą grę powtórzono 200 razy po pięć rund tak, aby
zminimalizować czynnik losowości
i umożliwić sprawdzenie, kto statystycznie częściej wygrywa.
Następnie sporządzono trzy wykresy
ilustrujących wyniki tych rozgrywek. Rys. 4.6 przedstawia każdą kombinację gier 
z przypisaną liczbą 
wygranych każdemu z modeli. Rys. 4.7 i 4.8 mają za zadanie pokazać średnią wygrywanych i przegrywanych
pul przez graczy, a rys. 4.9 pokazuje rozkład wykonanych akcji. Takie wykresy pozwolą stwierdzić, który z modeli częściej wygrywa, ale też częściej
ryzykuje, przegrywając więcej, a który gra ostrożniej.


\begin{figure}[!ht]
  \centering
  \hspace*{-1cm}   
  \includegraphics[width=0.9\textwidth]{./img/mecze.png}
  \caption{Kombinacje rozgrywek między utworzonymi modelami.}
\end{figure}


\vspace{3cm}
Analizując rys. 4.6 można zaóważyć, że model 1 wygrywał z resztą utworzonych AI najczęściej oprócz
nr 2.
Najwięszą przewagę 
zdobył względem modelu numer 4, około 130 wygranych względem 70 porażek oraz około 118 zwycięstw z
nr 5. Rozgrywki z resztą
uczestinków, 
cechuje się niewielkimi przewagami gracza. Przy wielokrotnym powtarzaniu eksperymentu nr 3 i
nr 2 mogł by się okazać lepsze przez występujący w grze czynnik losowości, który ma wpływ na
uzyskiwane karty.

Kolejny obiekt zdobywający najlepsze wyniki
to nr 3. Osiągnął on tak samo dużą różnicę między wygranymi, a przegranymi z modelem nr 4.
Na podstawie takich informacji można stwierdzić, że AI powstałe po 40 iteracjach przegrywa
najczęściej,
a
utworzone w 1 i 3 etapie najżadziej. W
dalszej cześci rozdziału zostanie zbadana możliwa przyczyna takich wyników.
Powtarzając eksperyment uzyskiwano podobne rezultaty z niewielkimi zmianami, jak więsza
częstotliwość zwycięstw modelu nr 2 i nr 3 względem nr 1 lub więszka liczba wygranych AI nr 5. 

Problemem wykresu rys. 4.6 jest pokazywanie tylko częstotliwości zwycięstw modeli ale nie zawiera
informacji o sposobie gry każdego z graczy. 
Taka wiedza pozwoliła by na stwierdzenie możliwych przyczyn powstałych
wyników.
W tym celu zebrano wszystkie wygrane wartości przez graczy z każdej rundy, a następnie
obliczono ich średnie. Rys. 4.7 przedstawia uśrednione wyniki przegrywanych pulli przez każdego z
graczy, rys. 4.8 prezentuje odwrotną cechę. Analizując oba rysunki, można zaóważyć, że model
nr 1, który okazał się wcześniej najlepszym AI, wygyrywa i traci najwięcej w rundach, ale też 
najczęściej wygyrwa. Rys. 4.9 pokazuje też, że model nr 1 wykonuje tak samo często akcję \emph{call}
i \emph{fold} oraz bardzo rzadko akcję \emph{fold}. Jest
najbardziej zrównoważonym graczem, który uzyskuje przy tym najbardziej skrajne wyniki. Zaletą modelu jest to, że 
średnie wartości są na podobnym poziomie, co oznacza, że wygrywa i tyle samo traci.
Gracz nr 3, który równie często wygrywał, 
osiąga na tych wykresach gorsze wyniki. Jego średnia wartość przegrywanej puli jest dużo wieksza
niż
wygrywana.  Model nr 5 gra podobnie do nr 1 co widać na rys. 4.9. Też ma równy rozkład wykonanych 
akcji \emph{call} i \emph{raise}.
Dodatkowo jego częstotliwośc wygyrwania przedstawiona na rys. 4.6 pokazuje, że nie uzyskuje on
tak dobrych wyników jak nr 1. AI nr 2 gra najmniej agresywnie, najczęściej wykonywał \emph{call}.
Może to być powód tak niskiej wygyranej puli.
Najgorze wyniki ma gracz nr 4, który też najczęściej pasował. Uzyskał
on znacznie większą średnia wartość wygranej względem przegranej przez taką strategię.


\vspace{1cm}



\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.87\textwidth]{./img/mecze_ps.pdf}
  \caption{Wyniki przegrywanych pul przez modele.}
\end{figure}


\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.87\textwidth]{./img/mecze_pw.pdf}
  \caption{Wyniki wygrywanych pul przez modele.}
\end{figure}


\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.87\textwidth]{./img/akcje.pdf}
  \caption{Rozkład wybieranych akcji przez modele.}
\end{figure}

\vspace{2cm}
\begin{table}
\centering
\caption{Dokładne wyniki uśrednionych puli zdobywanych lub traconych przez modele.}
\begin{tabular}{|c|c|c| }
   \hline
   model & średnia wygrana & średnia przegrana \\
    \hline
   1 & 61 & 65 \\ 
   \hline
   2 & 49 & 52  \\  
   \hline
   3 & 53 & 58 \\
   \hline
   4 & 61 & 50 \\
   \hline
   5 & 60 & 61 \\
   \hline
\end{tabular}
\end{table}



\vspace{20cm}
\section{Porównanie modeli z graczem nieblefującym}

Końcowym etapem oceny jakości algorytmu Deep CFR jest wykonanie gry między najlepszymi utworzonymi
modelami, a graczem który nie blefuje. Jest to program symulujący grę w HULH, wykonując tylko 
akcje opierające się na sile dostępnych kart.
W tym celu wykorzystano funkcję zawartą w bibliotece PyPokerEngine -
\emph{estimate\_hole\_card\_win\_rate}. Wykonuje ona określoną liczbę iteracji mozliwych wersji gier, a
nastepnie liczy szanse wygrania z dostepnymi kartami. W zalezności od zwracanej wartości
nastepnie wykonuje akcję \emph{fold} lub \emph{call}. 

Rozegrano 2 gry po 200 powtórzeń, gdzie każda z nich składała się z 5 rund. Na rys. 4.9, 4.10, 4.11 zaprezentowano
wyniki uzyskanych rozgrywek. Pierwszy z wykresów przedstawia częstotliwośc wygrywania modeli z
graczem szczerym (HP). W przypadku nr 3 liczba zwycięstw jest równa liczbie porażek. Drugi z
sztucznych inteligencji uzyskał nieznacznie lepsze wyniki. Dodatkowo analizując następne wykresy
można zauwazyć, że gracz HP przegrywa znacznie wiekszą średnią pulę względem modeli ale też wygrywa 
większą stawkę. Oznacza to, że po mimo braku wykonywanej akcji \emph{raise} program częściej
ryzykuje.


\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.6\textwidth]{./img/honest1.pdf}
  \caption{Wyniki rozgrywek między modelami nr 1, nr 3 i graczem nieblefującym.}
\end{figure}



\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\textwidth]{./img/honest2.pdf}
  \caption{Wyniki przegrywanych pul między modelami nr 1, nr 3 i graczem nieblefującym.}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\textwidth]{./img/honest3.pdf}
  \caption{Wyniki wygrywanej pul między modelami nr 1, nr 3 i graczem nieblefującym.}
\end{figure}


\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\textwidth]{./img/honest4.pdf}
  \caption{Rozkład wykonywanych akcji przez modele w trakcie rozgrywek.}
\end{figure}

\section{Podsumowanie}


Rozdział przedstawił wyniki utworzonych AI. Rezultaty okazały się mało optymistyczne. Model
wytrenowany w ostatniej iteracji nie uzyskuje najlepszych wyników, a przedostanie AI przegrywa
najcześciej. 


\chapter{Podsumowanie}

Praca przedstawiła problem uczenia przez wzmacnianie w środowiskach częściowo obserwowalnych. 
Rozdziały 1 i 2 omówiły źródło takich problemów. Z tego powodu należy 
korzystać ze skomplikowanych algorytmów, które potrafią powiązać obserwacje z najlepszymi akcjami w
danym stanie. W przypadku gier karcianych w tym celu korzysta się z algorytmów bazujących na
metodzie CFR. 

Zaimplementowany algorytm w niniejszej pracy, Deep CFR usprawnił proces znany w metodzie CFR,
przez użycie sieci
neuronowych. Taka konstrukcja przyspiesza zbieganie się AI w dużych grach jak HULH
do Równowagi Nasha \cite{DCFR}.
Program użyty w pracy nie pozwolił na określenie stopnia bliskości
do takiego
stanu. Wynika to z faktu, że algorytmy używające metody CFR korzystają z metryki
\emph{Exploaibility} bazującej na wartości BR (\emph{Best Response})  do śledzenia postępów wybieranych
strategii przez AI \cite{lbr}.
Taki element głównie się implementuje w grach
abstrakcyjnych jak \emph{Kuhn Poker} z powodu bardzo dużej obciążalności obliczeniowej. 
Duże gry stosują mniej dokładną metryki jak LBR(\emph{Local Best Response}) lub
RL\_BR (\emph{RL Best Response}), które z dużym przybliżeniem zwracają bliskość
modelu do Równowagi Nasha \cite{lbr} \cite{RL}. Charakteryzują się one dużym skomplikowaniem 
implementacyjnym.

 
Z tego powodu do oceny jakości Deep CFR użyto prostszej metody.
Rozegrano wiele gier HULH między utworzonymi modelami, a następnie przeanalizowano wyniki
i wybrano
najlepszego z nich na bazie uzyskanych cech. Rezultaty okazały się mało optymistyczne w porównaniu
do czasu poświęconego na czas uczenia modeli.
Aktualny rozdział przedstawia możliwe przyczyny takich efektów oraz wnioski po dokonaniu wszystkich poprzednich etapów
pracy wraz z
mozliwymi poprawami parametów. Końcowa cześć pracy przedstawi dalszą historią algorytmu Deep CFR.
Dodatkowo zostanie przedstawiony 
możliwy
kierunek rozwoju uczenia maszynowego w środowiskach częściowo-obserwowalnych.





\section{Wnioski}

Gra Limit Poker Texas Hold'em jest bardzo trudnym środowiskiem do uczenia sztucznych
inteligencji, wynika to z częściowej obserwowalności. 
Z tego powodu model Cepheus, zdolny pokonywać ludzi w takim środowisku powstał 
dopiero w 2015 roku. Od tamtej pory rozpoczął się nagły rozwój metod uczenia maszynowego 
coraz większych gier 
karcianych.
Przykładami są DeepStack lub Libratus rozwiązujące grę No Limit Texas Poker Hold'em i pokonujące
profesjonalnych ludzi.

Głównym zadaniem niniejszej pracy była próba zmierzenia się z takim środowiskiem. W celu uproszczenia
zadania wybrano grę HULH. Następnie zaimplementowano 
nowoczesny algorytm Deep CFR i wytrenowano pięć modeli
rozpoznawania. Pierwszy problem, jaki napotkano to, powolna nauka sieci $\theta_{p}$,
wraz z dużym błędem predykcji. Podczas uczenia wartości nie spadały poniżej 600.
Możliwym rozwiązaniem jest zwiększenie parametru \emph{learning rate} oraz
dopracowanie architektury sieci neuronowej. Dodatkową przyczyną takich rezultatów mogą być
zbiory danych o słabej jakości. Prawdopodobnie nauka by przebiegała lepiej przez zwiększenie
zawartości obserwowalnych informacji wejściowych przez np. liczbę żetonów w grze. W taki sposób
sieć neuronowa by miała więcej informacji, które by zostały użyte do zwracania właściwego 
wektora żalu.

Kolejnym problemem, który może mieć duże
znaczenie w grze to konstrukcja wprowadzanych informacji do modelu.
Dane wejściowe użyte w implementacji to karty widziane od strony gracza oraz historia 
z jednej rundy. Wadą takiej architektury jest to, że w praktyce rozgrywki Poker Texas Hold'em 
mogą odbywać się dłużej. Wtedy gracz musi dodatkowo używać takich informacji jak, wybrana strategia
przeciwnika w poprzednim etapie, czy grał ostrożnie, agresywnie albo blefował. Kolejnym czynnikiem jest 
sposób zmieniania się gry zależnie od liczby pozostałych żetonów w puli oraz od numeru rundy. Gracze
mogą podejmować bardziej ryzykowne i nierozważne ruchy, będąc w stanie bliskim porażki. Takie
informacje mogłyby być kluczowe w osiągnięciu zwycięstwa.
Prawdopodobnie przy uwzględnieniu tych elementów, utworzone modele lepiej by
dobierały strategie do określonych stanów gry. To by wymagało wykonywania eksploracji MCCFR ES na znacznie większych drzewach decyzyjnych
obejmujących wiele rund. Dodatkowo dane wejściowe sieci neuronowej byłyby większe.
W takim przypadku
możliwym zbiorem informacji mógłby być zestaw składający się z widocznych kart, historii z wielu
rund, liczby żetonów każdego z graczy oraz numeru gry. W taki sposób model nauczyłby się 
lepiej dostosowywać sposób gry do obserwacji. 


Deep CFR spełnił funkcję i utworzył modele, które wygrywają z prostymi programami symulującymi
grę Poker Texas Hold'em jak przedstawiony gracz nieblefujący, HP w rozdziale 3. Pomimo dobrych
rezultatów dużym problemem okazał się
proces powstawiania sztucznych inteligencji. Pozornie można by było oczekiwać, że
algorytm będzie tworzył lepsze AI wraz z dłuższym czasem działania. W pracy doszło do odwrotnej
sytuacji. Sztuczne inteligencje utworzone w iteracjach 10 i 40 wygrywały z późniejszymi obiektami.
Ciężko określić przyczynę takich rezultatów. Pomocne okazałoby się użycie metryki
$\emph{Exploiability}$ do śledzenia postępów AI pomimo zwiększenia wymaganej mocy obliczeniowej
przez algorytm. Taki element pozwolił by na stwierdzenie czy algorytm ominął pónkt o najlepszej
jakości i w dalszym procesie uzyskuje podobne lub coraz gorze efekty. Pozwoliło by to na zatrzymanie procesu 
uzyskując najlepszy możliwy model z implementacji.







\section{Dalszy rozwój algorytmów bazujących na metodzie CFR}

Algorytm powstały w 2017 roku dawał dobre rezultaty, ale przez wykorzystanie dwuch sieci neuronowych
tworzył duzą wariancję wyników \cite{SD-CFR}. Z tego powodu powstał jego następca Single Deep CFR.
Po wykonanych testach uzyskał nie znacznie lepsze wyniki. Algorym dalej nie był perfekcyjny, z tego
powodu w 2020 roku powstała metoda uczenia maszynowego o nazwie DREAM \cite{DREAM}. Są to bardzo
dobre sposoby na wtorzenie AI w grach karcianych. Po mimio tego ich wadą jest to, że wymagają od gry
aby wartość wygranej i przegranej sumowała się do zero. Taką cechą okresla się środowiska Zero-sum
\cite{gt}. Przez to algorytmy są niemożliwe do użycia w wielu środowiskach.  
Kolejnym problemem tych
metod jest przeprowadzenie testów schodzenia się do Równowagi Nasha tylko w grach 2-osobowych \cite{DCFR}.
Wiele środowisk jak Poker Texas Hold'em standardowo uwzględnia wiekszą ilosć uczestników. Na
podstawie takich informacji można stwierdzić, że zimplementowany Deep CFR wraz z jego następcami nie
wyczerpały tematu środowisk gier karcianych. Nawet najnowsze AI, które pokonują profesjonalnych
graczy jak DeepStack lub Libratus muszą trzymać sie tego warunku \cite{libratus} \cite{ds}. 
Oznacza to, że takie gry to bardzo trudne środowiska, które prawdopodobnie będą jeszcze
długo badane pod względem możliwych rozwiązań.





\begin{thebibliography}{100}
   \bibitem{hist AI} Haenlein, Michael, and Andreas Kaplan. "A brief history of artificialintelligence: On the past, present, and future of artificial intelligence." California management review 61.4 (2019): 5-14.
   \bibitem{Go} Gibney, Elizabeth. "Google AI algorithm masters ancient game of Go." Nature News 529.7587 (2016): 445.
   \bibitem{Dota2} Berner, Christopher, et al. "Dota 2 with large scale deep reinforcement learning." arXiv preprint arXiv:1912.06680 (2019).
   \bibitem{iig} Brown, Noam, et al. "Combining deep reinforcement learning and search for imperfect-information games." arXiv preprint arXiv:2007.13544 (2020).

   \bibitem{DCFR} Brown, Noam, et al. "Deep counterfactual regret minimization." International conference on machine learning. PMLR, 2019. 
   \bibitem{XFP} Heinrich, Johannes, Marc Lanctot, and David Silver. "Fictitious self-play in extensive-form games." International conference on machine learning. PMLR, 2015.
   \bibitem{CFR} Zinkevich, Martin, et al. "Regret minimization in games with incomplete information." Advances in neural information processing systems 20 (2007): 1729-1736.
   \bibitem{NFSP} Heinrich, Johannes, and David Silver. "Deep reinforcement learning from self-play in imperfect-information games." arXiv preprint arXiv:1603.01121 (2016).

   \bibitem{poker} Teófilo, Luís Filipe Guimarães. "Building a poker playing agent based on game logs using supervised learning." (2010).
   \bibitem {theory poker} Bouju, Gaëlle, et al. "Texas hold'em poker: a qualitative analysis of gamblers' perceptions." Journal of Gambling Issues 28 (2013): 1-28.
   \bibitem{class} Félix, Dinis Alexandre Marialva. "Artificial intelligence techniques in games with incomplete information: opponent modelling in Texas Hold'em." (2008).
   \bibitem{mdp} Xiang, Xuanchen, and Simon Foo. "Recent Advances in Deep Reinforcement Learning Applications for Solving Partially Observable Markov Decision Processes (POMDP) Problems: Part 1—Fundamentals and Applications in Games, Robotics and Natural Language Processing." Machine Learning and Knowledge Extraction 3.3 (2021): 554-581.

   \bibitem{rn}  Nogal-Meger, P. (2012). Dylemat więźnia jako przykład wykorzystania teorii gier. Prace i Materiały Wydziału Zarządzania Uniwersytetu Gdańskiego, 10(4, cz. 2), 87–95.
   \bibitem{gt} Myerson, Roger B. Game theory. Harvard university press, 2013.
   \bibitem{cepheus} Bowling, Michael, et al. "Heads-up limit hold'em poker is solved." Communications of the ACM 60.11 (2017): 81-88.
   \bibitem{libratus} Brown, Noam, and Tuomas Sandholm. "Superhuman AI for heads-up no-limit poker: Libratus beats top professionals." Science 359.6374 (2018): 418-424.
   \bibitem{ds} Moravčík, Matej, et al. "Deepstack: Expert-level artificial intelligence in heads-up no-limit poker." Science 356.6337 (2017): 508-513. 
   \bibitem{ss} Davis, Trevor, Neil Burch, and Michael Bowling. "Using response functions to measure strategy strength." Twenty-Eighth AAAI Conference on Artificial Intelligence. 2014. 
   \bibitem{mccfr} Lanctot, Marc, et al. "Monte Carlo sampling for regret minimization in extensive games." Advances in neural information processing systems 22 (2009): 1078-1086.
   \bibitem{early} Prechelt, Lutz. "Early stopping-but when?." Neural Networks: Tricks of the trade. Springer, Berlin, Heidelberg, 1998. 55-69.

   \bibitem{lbr} Lisy, Viliam, and Michael Bowling. "Eqilibrium approximation quality of current no-limit poker bots." Workshops at the Thirty-First AAAI Conference on Artificial Intelligence. 2017.
   \bibitem{RL} Eric Steinberger (2019). PokerRL in 
   \text{https://github.com/TinkeringCode/PokerRL} GitHub.GitHub repository.
   \bibitem{numpy} Harris, C.R., Millman, K.J., van der Walt, S.J. et al. Array programming with NumPy. Nature 585, 357–362 (2020)
   \bibitem{SD-CFR} Steinberger, Eric. "Single deep counterfactual regret minimization." arXiv preprint arXiv:1901.07621 (2019).
   \bibitem{DREAM} Steinberger, Eric, Adam Lerer, and Noam Brown. "DREAM: Deep regret minimization with advantage baselines and model-free learning." arXiv preprint arXiv:2006.10410 (2020).
   \bibitem{tensorflow} Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,
Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis,
Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,
Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia,
Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Mike Schuster,
Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens,
Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,
Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas,
Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke,
Yuan Yu, and Xiaoqiang Zheng.
TensorFlow: Large-scale machine learning on heterogeneous systems,
2015. Software available from tensorflow.org.
\end{thebibliography}



\end{document}

